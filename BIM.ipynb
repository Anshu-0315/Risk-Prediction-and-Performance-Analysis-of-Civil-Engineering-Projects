{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing and Model Building**\n",
        "\n",
        "This segment of code is responsible for preparing the raw data for machine learning. It loads the dataset, identifies and separates the features from the target variable, and performs necessary transformations. Specifically, it handles the categorical variables by applying one-hot encoding, which converts them into a numerical format that the GradientBoostingClassifier can process. The final step is splitting the data into a training set and a testing set, a critical step for a robust model evaluation."
      ],
      "metadata": {
        "id": "Q0RVH9Q6ayal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('bim_ai_civil_engineering_dataset.csv')\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df.drop('Risk_Level', axis=1)\n",
        "y = df['Risk_Level']\n",
        "\n",
        "# Drop columns that are not useful for prediction\n",
        "X = X.drop(['Project_ID', 'Start_Date', 'End_Date'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "categorical_cols_to_encode = ['Project_Type', 'Location', 'Weather_Condition']\n",
        "\n",
        "# Perform one-hot encoding on categorical features\n",
        "X = pd.get_dummies(X, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N87A62mZax97",
        "outputId": "ea258764-e0e3-4683-db6c-9e39566ec8fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 700 samples\n",
            "Testing set size: 300 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training and Evaluation**\n",
        "\n",
        "This segment focuses on the machine learning pipeline. It first initializes and trains the GradientBoostingClassifier model using the prepared training data. After the model has learned the patterns in the data, it is used to make predictions on the unseen testing data. Finally, the model's performance is rigorously evaluated by calculating the prediction accuracy and generating a confusion matrix. The confusion matrix provides a detailed breakdown of correct and incorrect predictions for each risk category."
      ],
      "metadata": {
        "id": "1tBzDsUid8bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Initialize and train the GradientBoostingClassifier model\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nPrediction Accuracy on Test Set: {accuracy:.4f}\")\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=gb_model.classes_, yticklabels=gb_model.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03n7kGQed7lC",
        "outputId": "e2044d47-4eef-488f-a1f4-4f147cc6d72a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction Accuracy on Test Set: 0.9433\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.96      0.98      0.97       163\n",
            "         Low       0.95      0.88      0.92        43\n",
            "      Medium       0.91      0.90      0.91        94\n",
            "\n",
            "    accuracy                           0.94       300\n",
            "   macro avg       0.94      0.92      0.93       300\n",
            "weighted avg       0.94      0.94      0.94       300\n",
            "\n"
          ]
        }
      ]
    }
  ]
}